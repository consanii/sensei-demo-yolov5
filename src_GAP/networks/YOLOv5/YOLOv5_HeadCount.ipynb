{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAvh_Mix2ESc",
        "outputId": "ea82ad2e-6f67-4ca6-f8d1-e0686194f9f8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "img_size = 640\n",
        "drive_path = \"/content/gdrive/MyDrive/Colab Notebooks/YOLOv5_HeadCount\"\n",
        "result_folder = f\"yolov5p_results_{img_size}x{img_size}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdRhABQLRlpz"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl_XK04IRhU5",
        "outputId": "9cc260df-c5ac-4446-e4f4-3229488c1917"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Install YOLOv5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie5uLDH4uzAp",
        "outputId": "aaf07bf9-59b5-4d5a-d408-9c7797d22d5b"
      },
      "outputs": [],
      "source": [
        "# clone YOLOv5 repository\n",
        "%cd /content\n",
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "d4627202-55e1-4d6a-fe5c-5ff2fb694749"
      },
      "outputs": [],
      "source": [
        "# install dependencies as necessary\n",
        "%cd /content/yolov5\n",
        "!pip install -r requirements.txt\n",
        "!pip uninstall wandb -qy  # deprecated dependency\n",
        "import torch\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDeebwqS9JbZ"
      },
      "source": [
        "## Step 6: Download a Dataset\n",
        "\n",
        "Run the code below to authenticate with Roboflow and download the dataset. Follow the link to generate an authentication token.\n",
        "\n",
        "Alternatively, provide an API key like so: `rf = Roboflow(api_key=...)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxpTCstRTcQZ"
      },
      "source": [
        "> ðŸŸ¢ **Tip:** The examples below work even if you use our non-custom dataset. However, you won't be able to deploy the model to Roboflow. To do that, create a custom dataset as described above or fork (copy) one into your [workspace](https://app.roboflow.com/) from [Universe](https://universe.roboflow.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knxi2ncxWffW",
        "outputId": "e4ada60c-feae-41e9-c595-ac252189004c"
      },
      "outputs": [],
      "source": [
        "%cd /content/yolov5\n",
        "!pip install -q roboflow==1.1.48\n",
        "\n",
        "import roboflow\n",
        "# roboflow.login()\n",
        "\n",
        "# b3fb8cbb-5fbb-4d36-beb8-79f172ab711a\n",
        "\n",
        "rf = roboflow.Roboflow(api_key=\"D4r51LraewNehHFSdeil\")\n",
        "project = rf.workspace(\"carlos-alberto-castro-zuleta-cnopi\").project(\"dataset-human-head\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUOiNLtMP5aG"
      },
      "source": [
        "# Train Custom YOLOv5 Detector\n",
        "\n",
        "### Next, we'll fire off training!\n",
        "\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** set the path to our yaml file\n",
        "- **cfg:** specify our model configuration\n",
        "- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n",
        "- **name:** result names\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir \"{drive_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_KGccTZZDMP"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", message=\"`torch.cuda.amp.autocast(args...)` is deprecated.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NcFxRcFdJ_O",
        "outputId": "df5b34c0-1194-45b3-dfbe-b284e3342747"
      },
      "outputs": [],
      "source": [
        "from yolov5 import train\n",
        "import os\n",
        "\n",
        "checkpoint = f\"{drive_path}/{result_folder}/weights/last.pt\"\n",
        "# Check if checkpoint exists\n",
        "if os.path.exists(checkpoint):\n",
        "    print(f\"Checkpoint found at {checkpoint}\")\n",
        "else:\n",
        "  checkpoint = None\n",
        "\n",
        "\n",
        "train.run(\n",
        "    imgsz=img_size,\n",
        "    batch=16,\n",
        "    epochs=200,\n",
        "    data=f\"{dataset.location}/data.yaml\",\n",
        "    weights=\"yolov5n.pt\",\n",
        "    project=drive_path,\n",
        "    name=result_folder,\n",
        "    resume=checkpoint,\n",
        "    cfg=f\"{drive_path}/yolov5p.yaml\",\n",
        "    # cache=\"ram\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVs_4zEeVbF"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C60XAsyv6OPe"
      },
      "outputs": [],
      "source": [
        "from utils.plots import plot_results  # plot results.txt as results.png\n",
        "plot_results(f'{drive_path}/{result_folder}/results.csv')\n",
        "Image(filename=f'{drive_path}/{result_folder}/results.png', width=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3qM6T0W53gh"
      },
      "source": [
        "# Run Inference With Trained Weights\n",
        "\n",
        "Next, we can run inference with a pretrained checkpoint on all images in the `test/images` folder to understand how our model performs on our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SyOWS80qR32"
      },
      "outputs": [],
      "source": [
        "%ls \"{drive_path}/{result_folder}/weights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nmZZnWOgJ2S"
      },
      "outputs": [],
      "source": [
        "# %cd /content/yolov5/\n",
        "# !python detect.py --weights \"{drive_path}/{result_folder}/weights/best.pt\" --img {img_size} --conf 0.35 --source {dataset.location}/test/images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2GCfQ7Xlzen"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlcq5YZXlLGp"
      },
      "outputs": [],
      "source": [
        "# %cd /content/yolov5/\n",
        "# !python detect.py --weights \"{drive_path}/{result_folder}/weights/best.pt\" --img {img_size} --conf 0.35 --source \"{drive_path}/samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odKEqYtTgbRc"
      },
      "outputs": [],
      "source": [
        "# import glob\n",
        "# from IPython.display import Image, display\n",
        "\n",
        "# for imageName in glob.glob('/content/yolov5/runs/detect/exp4/*.jpg')[:10]: #assuming JPG\n",
        "#     display(Image(filename=imageName))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HcVFjj5M9xj"
      },
      "outputs": [],
      "source": [
        "!pip install onnx onnx_opcounter onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_Nf4lGVDr2C"
      },
      "outputs": [],
      "source": [
        "%cd /content/yolov5/\n",
        "!python export.py \\\n",
        "  --imgsz {img_size} \\\n",
        "  --batch-size 1 \\\n",
        "  --data {dataset.location}/data.yaml \\\n",
        "  --weights \"{drive_path}/{result_folder}/weights/best.pt\"  \\\n",
        "  --simplify \\\n",
        "  --optimize \\\n",
        "  --include onnx tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLUfqBmq92qu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Step 8: Convert the TensorFlow model to TensorFlow Lite with optimization (quantization)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(f\"{drive_path}/{result_folder}/weights/best_saved_model\")\n",
        "\n",
        "# Apply post-training quantization to reduce model size\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Default quantization\n",
        "\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Step 9: Save the optimized TensorFlow Lite model\n",
        "with open(f\"{drive_path}/{result_folder}/weights/best.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"Optimized TensorFlow Lite model saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51-I_u4MLgAy"
      },
      "outputs": [],
      "source": [
        "# Load ONNX model\n",
        "import onnx\n",
        "from onnx_opcounter import calculate_params, calculate_macs\n",
        "\n",
        "onnx_model_path = f\"{drive_path}/{result_folder}/weights/best.onnx\"\n",
        "model = onnx.load(onnx_model_path)\n",
        "\n",
        "print(f\"Parametes: {calculate_params(model)*1E-3:.2f} K\")\n",
        "print(f\"Operations: {calculate_macs(model)*1E-6:.2f} MFLOPs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeIsaHZYKJGn"
      },
      "outputs": [],
      "source": [
        "image_path = dataset.location + \"/test/images/PartB_00041_jpg.rf.36e52203a9ae48816323eeacdd82b1ff.jpg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkOlln8RDM5F"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "class TFLiteModel:\n",
        "    def __init__(self, model_path: str):\n",
        "        self.interpreter = tf.lite.Interpreter(model_path)\n",
        "        self.interpreter.allocate_tensors()\n",
        "\n",
        "        self.input_details = self.interpreter.get_input_details()\n",
        "        self.output_details = self.interpreter.get_output_details()\n",
        "\n",
        "    def predict(self, *data_args):\n",
        "        assert len(data_args) == len(self.input_details)\n",
        "        for data, details in zip(data_args, self.input_details):\n",
        "            self.interpreter.set_tensor(details[\"index\"], data)\n",
        "        self.interpreter.invoke()\n",
        "        return self.interpreter.get_tensor(self.output_details[0][\"index\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX9ksX6KR7b9"
      },
      "outputs": [],
      "source": [
        "%cd /content/yolov5/\n",
        "from utils.general import non_max_suppression\n",
        "\n",
        "model = TFLiteModel(f\"{drive_path}/{result_folder}/weights/best.tflite\")\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.resize(image, (img_size, img_size))\n",
        "image = image.astype(np.float32)[np.newaxis]\n",
        "image = (image) / 255\n",
        "\n",
        "prediction = model.predict(image)\n",
        "print(\"Predictions\")\n",
        "print(prediction)\n",
        "\n",
        "prediction_torch = torch.tensor(prediction)\n",
        "label = non_max_suppression(prediction_torch, conf_thres=0.3)\n",
        "print(\"Labels\")\n",
        "print(label)\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.resize(image, (img_size, img_size))\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for x1, y1, x2, y2, confidence, class_id in label[0]:\n",
        "      # print(x1, y1, x2, x2, confidence, class_id)\n",
        "\n",
        "      # center_x, center_y, width, height\n",
        "      x1 = int(x1*img_size)\n",
        "      y1 = int(y1*img_size)\n",
        "      x2 = int(x2*img_size)\n",
        "      y2 = int(y2*img_size)\n",
        "\n",
        "      cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "# Display image\n",
        "cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjI1SX7uWRzX"
      },
      "outputs": [],
      "source": [
        "# Go thourgh all folder in drive_path and print the mAP50, mAP50-95, Parameter and OP numbers\n",
        "%cd {drive_path}\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "directories = os.listdir(drive_path)\n",
        "# Sort list\n",
        "directories = sorted(directories)\n",
        "\n",
        "for folder in directories:\n",
        "  if folder.startswith(\"yolov5p_results\"):\n",
        "    print(folder)\n",
        "\n",
        "    onnx_model_path = f\"{drive_path}/{folder}/weights/best.onnx\"\n",
        "    model = onnx.load(onnx_model_path)\n",
        "    print(f\"- Operations: {calculate_macs(model)*1E-6:.2f} MFLOPs\")\n",
        "    print(f\"- Parametes : {calculate_params(model)*1E-3:.2f} K\")\n",
        "    # Get best mAP50 using pandas\n",
        "    df = pd.read_csv(f\"{drive_path}/{folder}/results.csv\")\n",
        "    df.columns = df.columns.str.strip()\n",
        "    print(f\"- mAP50     : {df['metrics/mAP_0.5'].max()*100:.2f} %\")\n",
        "    print(f\"- mAP50-95  : {df['metrics/mAP_0.5:0.95'].max()*100:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkLj-_zYusuk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
