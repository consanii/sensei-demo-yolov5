{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from nntool.api import NNGraph\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level = logging.ERROR)\n",
        "import cv2\n",
        "import glob\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "dimension = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = NNGraph.load_graph(f\"YOLOv5_HeadCount_{dimension}x{dimension}.tflite\", load_quantization = False)\n",
        "model.name = \"YOLOv5\"\n",
        "\n",
        "# Model show returns a table of information on the Graph\n",
        "model.adjust_order()\n",
        "print(model.show())\n",
        "\n",
        "print(f\"Total ops: {model.total_ops*1E-6:.0f} MFLOPs\")\n",
        "print(f\"Total parameters: {model.total_memory_usage[1]*1E-3:.0f} K Item\")\n",
        "print(f\"Total memory usage: {model.total_memory_usage[0]*1E-3:.1f} K Item\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The equivalent of the fusions --scale8 command. The fusions method can be given a series of fusions to apply\n",
        "model.fusions('scaled_match_group')\n",
        "\n",
        "# model.remove_nodes(node_from = model[\"CONCAT_0_253\"], node_to = None, up = False, leave = None)\n",
        "# model.remove_nodes(node_from = model[\"LOGISTIC_0_243\"], node_to = None, up = False, leave = True)\n",
        "\n",
        "# model.remove_nodes(node_from = model[\"CONCAT_0_233\"], node_to = None, up = False, leave = None)\n",
        "# model.remove_nodes(node_from = model[\"LOGISTIC_0_230\"], node_to = None, up = False, leave = True)\n",
        "\n",
        "model.adjust_order()\n",
        "print(model.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MyDataLoader():\n",
        "\n",
        "    def __init__(self, image_files, max_idx = None, transpose_to_chw = True):\n",
        "        self._file_list = image_files\n",
        "        self._idx = 0\n",
        "        self._max_idx = max_idx if max_idx is not None else len(image_files)\n",
        "        self._transpose_to_chw = transpose_to_chw\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._idx = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._idx >= self._max_idx:\n",
        "            raise StopIteration()\n",
        "        filename = self._file_list[self._idx]\n",
        "\n",
        "        # Here we read the image and make it a numpy array\n",
        "        image = Image.open(filename)\n",
        "        img_array = np.array(image)\n",
        "        #img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Apply some preprocessing\n",
        "        img_array = cv2.resize(img_array, (dimension, dimension))\n",
        "        img_array = img_array / 255.0\n",
        "        img_array = img_array.astype(np.float32)\n",
        "\n",
        "        # Detect if channel is the last or first\n",
        "        if img_array.shape[2] <= 3 and self._transpose_to_chw:\n",
        "            img_array = img_array.transpose(2, 0, 1)\n",
        "\n",
        "        self._idx += 1\n",
        "        return img_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The executer returns all the layer output. Each layer output is an array of the outputs from each output of a layer\n",
        "# Generally layers have one output but some (like a split for example) can have multiple outputs\n",
        "# Here we select the first output of the last layer which in a graph with one output will always be the the\n",
        "# graph output\n",
        "data_loader = MyDataLoader(glob.glob(\"input_images_HeadCount/*\"), transpose_to_chw = False)\n",
        "test_image = next(data_loader)\n",
        "\n",
        "print(test_image.shape)\n",
        "\n",
        "# Transpose for to HWC for imshow\n",
        "# show_image = test_image.transpose(1, 2, 0)\n",
        "plt.imshow(test_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run these cells if you want to quantize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistics = model.collect_statistics(data_loader)\n",
        "fig = statistics.plot_node_stats(model[0])\n",
        "fig = statistics.plot_node_stats(model[-1])\n",
        "fig.show()\n",
        "\n",
        "# fig = statistics.plot_all_stats()\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.quantize(\n",
        "    statistics,  #=None, # Already quantized in tflite --> use the statistics from the graph itself\n",
        "    graph_options = {\n",
        "        \"scheme\": \"SQ8\",\n",
        "        \"use_ne16\": True,\n",
        "        #            \"scheme\": \"FLOAT\",\n",
        "        #            \"float_type\": \"bfloat16\",\n",
        "        #            \"force_input_size\": 16,\n",
        "        #            \"force_output_size\": 16,\n",
        "        #            \"force_external_size\": 16,\n",
        "        #            \"weight_bits\": 8,\n",
        "        \"hwc\": True\n",
        "    },\n",
        ")\n",
        "print(model.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total ops: {model.total_ops*1E-6:.0f} MFLOPs\")\n",
        "print(f\"Total parameters: {model.total_memory_usage[1]*1E-3:.0f} KB\")\n",
        "print(f\"Total memory usage: {model.total_memory_usage[0]*1E-3:.1f} K Item\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a new image to repeat the inference\n",
        "data_loader._idx = 0\n",
        "test_image = next(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"execute quantized model with dequantizing data\")\n",
        "print(test_image.shape)\n",
        "output = model.execute(test_image, quantize = True, dequantize = True)\n",
        "\n",
        "print(output[-1][0][0].shape)\n",
        "\n",
        "# Print first 10 outputs\n",
        "for i, (x, y, w, h, confidence, class_id) in enumerate(output[-1][0][0]):\n",
        "    print(\"- %d: %f %f %f %f %f %f\" % (i, x, y, w, h, confidence, class_id))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(test_image)\n",
        "ax = plt.gca()\n",
        "\n",
        "label = output[-1][0]\n",
        "for x, y, w, h, confidence, class_id in label[0]:\n",
        "    if confidence > 0.4:\n",
        "\n",
        "        x, w, y, h = x * dimension, w * dimension, y * dimension, h * dimension\n",
        "         # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((x - w / 2, y - h / 2), w, h, linewidth = 1, edgecolor = 'b', facecolor = 'none')\n",
        "\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quant_execution = model.execute(test_image, quantize = True, dequantize = False)\n",
        "print([quant_execution[inp.step_idx][0].shape for inp in model.input_nodes()])\n",
        "\n",
        "# Print first 10 outputs\n",
        "for i, (x, y, w, h, confidence, class_id) in enumerate(quant_execution[-1][0][0]):\n",
        "    print(\"- %d: %d %d %d %d %d %d\" % (i, x, y, w, h, confidence, class_id))\n",
        "    if i == 10:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from nntool.api.utils import model_settings\n",
        "\n",
        "quant_execution = model.execute(test_image, quantize = True, dequantize = False)\n",
        "print([quant_execution[inp.step_idx][0].shape for inp in model.input_nodes()])\n",
        "\n",
        "print(len(quant_execution))\n",
        "\n",
        "input_tensors = [quant_execution[inp.step_idx][0] for inp in model.input_nodes()]\n",
        "print(input_tensors[0].shape)\n",
        "\n",
        "print(model.quantization[model.input_nodes()[0].name])\n",
        "print(model.quantization[model.output_nodes()[0].name])\n",
        "\n",
        "scale = model.quantization[model.output_nodes()[0].name].out_qs[0].scale[0]\n",
        "zero_point = model.quantization[model.output_nodes()[0].name].out_qs[0].zero_point[0]\n",
        "\n",
        "print(scale, zero_point)\n",
        "\n",
        "plt.figure()\n",
        "# plt.imshow(test_image)\n",
        "plt.imshow(input_tensors[0])\n",
        "\n",
        "# Get color order of the image\n",
        "ax = plt.gca()\n",
        "\n",
        "label = quant_execution[-1][0].astype(np.float32)\n",
        "label = (label - zero_point) * scale\n",
        "for i, (x, y, w, h, confidence, class_id) in enumerate(label[0]):\n",
        "    if confidence > 0.4:\n",
        "        x, w, y, h = x * dimension, w * dimension, y * dimension, h * dimension\n",
        "        # Create a Rectangle patch\n",
        "        rect = patches.Rectangle((x - w / 2, y - h / 2), w, h, linewidth = 1, edgecolor = 'b', facecolor = 'none')\n",
        "\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_dir = os.path.abspath(\"\")\n",
        "gen_dir = os.path.join(current_dir, f\"../../src/networks/YOLOv5/generated_{dimension}x{dimension}/\")\n",
        "\n",
        "# Normalized the path\n",
        "gen_dir = os.path.normpath(gen_dir)\n",
        "print(f\"Using generated directory: {gen_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dimension >= 448:\n",
        "  input_location = 'AT_MEM_L3_DEFAULTRAM'\n",
        "else:\n",
        "  input_location = 'AT_MEM_L2'\n",
        "\n",
        "res = model.gen_project(directory = gen_dir,\n",
        "                        input_tensors = [quant_execution[inp.step_idx][0] for inp in model.input_nodes()],\n",
        "                        settings = model_settings(graph_async_fork = True,\n",
        "                                                  l3_flash_device = 'AT_MEM_L3_MRAMFLASH',\n",
        "                                                  default_input_home_location = input_location,\n",
        "                                                  default_input_exec_location = input_location,\n",
        "                                                  l3_ram_ext_managed=True,\n",
        "                                                  tensor_directory = './weights_tensors'),\n",
        "                        at_loglevel = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    res = model.execute_on_target(directory = gen_dir,\n",
        "                                  input_tensors = [quant_execution[inp.step_idx][0] for inp in model.input_nodes()],\n",
        "                                  check_on_target = False,\n",
        "                                  settings = model_settings(l1_size = 128000,\n",
        "                                                            l2_size = 300000,\n",
        "                                                            graph_async_fork = True,\n",
        "                                                            l3_flash_device = 'AT_MEM_L3_MRAMFLASH',\n",
        "                                                            tensor_directory = './weights_tensors'),\n",
        "                                #   do_clean = True,\n",
        "                                  print_output = True,\n",
        "                                  at_loglevel = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gap9-sdk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
